import torch
from torch.utils.data import Dataset
from datasets import load_dataset

class VQARADDataset(Dataset):
    def __init__(self, data_dir, image_dir, transform=None, tokenizer=None, mode='train', answer_to_idx=None):
        """
        Args:
            data_dir: (å·²å¼ƒç”¨) ä¿ç•™æ˜¯ä¸ºäº†å…¼å®¹ train.py çš„è°ƒç”¨
            image_dir: (å·²å¼ƒç”¨) ä¿ç•™æ˜¯ä¸ºäº†å…¼å®¹ train.py çš„è°ƒç”¨
            transform: å›¾ç‰‡é¢„å¤„ç†
            tokenizer: BERT tokenizer
            mode: 'train' æˆ– 'test'
            answer_to_idx: ç­”æ¡ˆåˆ°ç´¢å¼•çš„æ˜ å°„å­—å…¸
        """
        self.transform = transform
        self.tokenizer = tokenizer
        self.mode = mode

        # ğŸš€ ä¿®æ”¹ç‚¹ 1: ä» Hugging Face åŠ è½½æ•°æ®é›†
        # å³ä½¿æœ¬åœ°æ²¡æœ‰æ•°æ®ï¼Œè¿™è¡Œä»£ç ä¹Ÿä¼šè‡ªåŠ¨ä¸‹è½½å¹¶ç¼“å­˜
        dataset = load_dataset("flaviagiammarino/vqa-rad")

        # ğŸš€ ä¿®æ”¹ç‚¹ 2: ä½¿ç”¨å®˜æ–¹çš„ train/test åˆ’åˆ†
        # HF æ•°æ®é›†æœ¬èº«å°±æœ‰ 'train' (1793æ¡) å’Œ 'test' (451æ¡)
        if mode == 'train':
            self.data = dataset['train']
        else:
            self.data = dataset['test']

        # æ„å»ºæˆ–ä½¿ç”¨ä¼ å…¥çš„ç­”æ¡ˆè¯è¡¨
        if answer_to_idx is None:
            self.answer_to_idx = self._build_answer_vocab()
        else:
            self.answer_to_idx = answer_to_idx

    def _build_answer_vocab(self):
        # ä»å½“å‰æ•°æ®é›†ä¸­æå–æ‰€æœ‰ç­”æ¡ˆæ„å»ºè¯è¡¨
        answers = [str(item['answer']).lower().strip() for item in self.data]
        vocab = {ans: idx for idx, ans in enumerate(set(answers))}
        return vocab

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # --- 1. Image å¤„ç† ---
        # ğŸš€ ä¿®æ”¹ç‚¹ 3: HF æ•°æ®é›†ç›´æ¥è¿”å› PIL Image å¯¹è±¡ï¼Œæ— éœ€è·¯å¾„è¯»å–
        image = item['image'].convert('RGB')

        if self.transform:
            image = self.transform(image)

        # --- 2. Question & Answer ---
        question = item['question']
        # ç»Ÿä¸€è½¬å°å†™å¹¶å»ç©ºæ ¼
        answer = str(item['answer']).lower().strip()

        # è·å–æ ‡ç­¾ï¼Œå¦‚æœä¸åœ¨è¯è¡¨ä¸­åˆ™å½’ä¸º <unk> (0)
        label = self.answer_to_idx.get(answer, 0)

        # --- 3. Answer Type æ¨æ–­ ---
        # ğŸš€ ä¿®æ”¹ç‚¹ 4: HF æ•°æ®é›†æ²¡æœ‰ answer_type å­—æ®µï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨æ¨æ–­
        # é€»è¾‘ï¼šå¦‚æœæ˜¯ yes/no é—®é¢˜ï¼Œåˆ™ä¸º Closed (0)ï¼Œå¦åˆ™ä¸º Open (1)
        if answer in ['yes', 'no']:
            type_id = 0  # Closed
        else:
            type_id = 1  # Open

        # --- Return ---
        if self.tokenizer:
            encoded_q = self.tokenizer(
                question,
                padding='max_length',
                truncation=True,
                max_length=32,
                return_tensors='pt'
            )
            return {
                'image': image,
                'input_ids': encoded_q['input_ids'].squeeze(),
                'attention_mask': encoded_q['attention_mask'].squeeze(),
                'label': torch.tensor(label, dtype=torch.long),
                'type_id': torch.tensor(type_id, dtype=torch.long)
            }
        else:
            return image, question, torch.tensor(label, dtype=torch.long), torch.tensor(type_id, dtype=torch.long)