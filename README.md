# Medical VQA on VQA-RAD: CNN-LSTM+SAN vs. VisualBERT

**åŸºäº VQA-RAD æ•°æ®é›†çš„åŒ»å­¦è§†è§‰é—®ç­”ï¼šCNN-LSTM+SAN ä¸ VisualBERT å¯¹æ¯”ç ”ç©¶**

## ğŸ“Œ Overview / é¡¹ç›®æ¦‚è¿°

This repository implements and compares two distinct architectures for the **VQA-RAD** (Medical Visual Question Answering) dataset. The project aims to bridge the gap between clinical imaging and natural language understanding using both traditional attention-based models and modern Transformer-based pre-trained models.

æœ¬é¡¹ç›®é’ˆå¯¹ **VQA-RAD**ï¼ˆåŒ»å­¦å½±åƒè§†è§‰é—®ç­”ï¼‰æ•°æ®é›†å®ç°å¹¶å¯¹æ¯”äº†ä¸¤ç§ä¸åŒçš„æ¶æ„ã€‚é¡¹ç›®æ—¨åœ¨åˆ©ç”¨ä¼ ç»Ÿæ³¨æ„åŠ›æœºåˆ¶æ¨¡å‹å’Œç°ä»£ Transformer é¢„è®­ç»ƒæ¨¡å‹ï¼Œç¼©å°ä¸´åºŠå½±åƒä¸è‡ªç„¶è¯­è¨€ç†è§£ä¹‹é—´çš„é¸¿æ²Ÿã€‚

------

## ğŸ—ï¸ Model Architectures / æ¨¡å‹æ¶æ„

### 1. CNN-LSTM + SAN (Stacked Attention Networks)

A classical dual-stream approach for VQA tasks:

- **Image Encoder**: Pre-trained ResNet152 to extract spatial visual features.
- **Text Encoder**: LSTM to process the sequence of the medical question.
- **Attention**: **Stacked Attention Networks (SAN)** perform multi-layer query-image reasoning to locate lesion areas related to the question.

ä¸€ç§ç»å…¸çš„ VQA åŒæµå¤„ç†æ–¹æ³•ï¼š

- **å›¾åƒç¼–ç å™¨**ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„ ResNet152 æå–ç©ºé—´è§†è§‰ç‰¹å¾ã€‚
- **æ–‡æœ¬ç¼–ç å™¨**ï¼šä½¿ç”¨ LSTM å¤„ç†åŒ»å­¦é—®é¢˜çš„åºåˆ—ä¿¡æ¯ã€‚
- **æ³¨æ„åŠ›æœºåˆ¶**ï¼š**å †å æ³¨æ„åŠ›ç½‘ç»œ (SAN)** é€šè¿‡å¤šå±‚â€œé—®é¢˜-å›¾åƒâ€æ¨ç†ï¼Œå®šä½ä¸é—®é¢˜ç›¸å…³çš„ç—…ç¶åŒºåŸŸã€‚

### 2. VisualBERT

A single-stream Transformer-based model:

- **Fusion Strategy**: Concatenates visual tokens (extracted via Faster R-CNN or Grid Features) with text tokens.
- **Self-Attention**: Automatically learns the implicit alignment between medical terms and radiological image regions through the Transformer layers.

ä¸€ç§åŸºäº Transformer çš„å•æµæ¨¡å‹ï¼š

- **èåˆç­–ç•¥**ï¼šå°†è§†è§‰ Tokenï¼ˆé€šè¿‡ Faster R-CNN æˆ–ç½‘æ ¼ç‰¹å¾æå–ï¼‰ä¸æ–‡æœ¬ Token ç›´æ¥æ‹¼æ¥ã€‚
- **è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼šé€šè¿‡ Transformer å±‚è‡ªåŠ¨å­¦ä¹ åŒ»å­¦æœ¯è¯­ä¸æ”¾å°„å½±åƒåŒºåŸŸä¹‹é—´çš„éšå¼å¯¹é½å…³ç³»ã€‚

------

## ğŸ“Š Dataset: VQA-RAD / æ•°æ®é›†ä»‹ç»

[flaviagiammarino/vqa-rad Â· Datasets at Hugging Face](https://huggingface.co/datasets/flaviagiammarino/vqa-rad)

VQA-RAD is a high-quality, manually labeled dataset by clinicians:

- **Modality**: CT, MRI, X-ray.
- **Anatomy**: Head, Chest, Abdomen.
- **Question Types**: Categorized into Closed-ended (Yes/No) and Open-ended (What, Where, How, etc.).

VQA-RAD æ˜¯ç”±ä¸´åºŠåŒ»ç”Ÿæ‰‹åŠ¨æ ‡æ³¨çš„é«˜è´¨é‡æ•°æ®é›†ï¼š

- **æ¨¡æ€**ï¼šåŒ…å« CTã€MRIã€X-rayã€‚
- **éƒ¨ä½**ï¼šå¤´éƒ¨ã€èƒ¸éƒ¨ã€è…¹éƒ¨ã€‚
- **é—®é¢˜ç±»å‹**ï¼šåˆ†ä¸ºå°é—­å¼ï¼ˆæ˜¯/å¦ï¼‰å’Œå¼€æ”¾å¼ï¼ˆä»€ä¹ˆã€åœ¨å“ªé‡Œã€å¦‚ä½•ç­‰ï¼‰ã€‚

------

## ğŸš€ Quick Start / å¿«é€Ÿå¼€å§‹

### 1. Requirements / ç¯å¢ƒè¦æ±‚

Bash

**Conda**

```
conda create -n vqa_env python=3.9
conda activate vqa_env
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y
```

Python

```
pip install -r requirements.txt
```

### 2. Training / è®­ç»ƒç¤ºä¾‹

**Train CNN-LSTM+SAN:**

Bash

```
python train_baseline.py
```

**Train VisualBERT:**

Bash

```
python train_visualbert.py
```

------

## ğŸ“ˆ Experimental Results / å®éªŒç»“æœ

| Comparative VisualBERT  vs. Baseline |                                      |                               |                 |
| :----------------------------------: | ------------------------------------ | ----------------------------- | --------------- |
|              **Metric**              | **CNN-LSTM+SAN (Baseline)**          | **VisualBERT (Advanced)**     | **Improvement** |
|         **Overall Accuracy**         | 41.20%                               | 49.20%                        | 8.00%           |
|         **Closed Accuracy**          | 54.20%                               | 64.10%                        | 9.90%           |
|          **Open Accuracy**           | 25.00%                               | 30.50%                        | 5.50%           |
|        **Model Architecture**        | ResNet152 + LSTM +SAN (From Scratch) | ResNet50 + BERT (Pre-trained) |                 |

------

## ğŸ“‚ Project Structure / ç›®å½•ç»“æ„

- `./data_loader.py`: Source code for data_loader. (æ•°æ®åŠ è½½æºä»£ç )
- `./model.py`: Source code for model. (æ¨¡å‹æºä»£ç )
- `./train_baseline.py`: Source code for train CNN-LSTM +SAN. (æ¨¡å‹è®­ç»ƒæºä»£ç )
- `./train_visualbert.py`: Source code for train VisualBERT. (æ¨¡å‹è®­ç»ƒæºä»£ç )
- `./requirements.txt`: env_txt. (æ‰€éœ€ç¯å¢ƒ)
- `weights/`: Saved model checkpoints. (æ¨¡å‹æƒé‡ä¿å­˜è·¯å¾„)
- `eval/`: Evaluation scripts for VQA accuracy and BLEU scores. (è¯„ä¼°æŒ‡æ ‡è®¡ç®—è„šæœ¬)

------
